{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9gigFV1NFYNM"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCtwSIqrFYNR"
   },
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUKfBqtM1gZs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"drive\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsy--2npFYNX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from corpus import Corpus\n",
    "#from lstm import Lstm\n",
    "from lstm_matrix import Lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "M0zXfQ7eFYNf",
    "outputId": "e2d57ec3-6ada-4dcc-d445-56d8f9a9dccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# use gpu if it is possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHZC6S2sFYNi"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "\n",
    "embed_size = 128     # Vector length to represent a word\n",
    "hidden_size = 1024   # hidden size of LSTM\n",
    "num_layers = 1      # only one layer\n",
    "num_epochs1 = 2      # num of iterations with lr = lr\n",
    "num_epochs2 = 4   # num of iterations with lr = lr*0.1 # 可以改成3\n",
    "num_epochs3 = 3     # num of iterations with lr = lr*0.05\n",
    "num_epochs4= 2     # num of iterations with lr = lr*0.02\n",
    "\n",
    "num_epochs = num_epochs1 +num_epochs2 +num_epochs3 + num_epochs4\n",
    "batch_size = 20     # Number of sentences in one batch (if too small: biger error due to randomness)\n",
    "seq_length = 25     # Number of  words in one sentence\n",
    "learning_rate = 0.01 #  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "JA21qpFDFYNp",
    "outputId": "1c19c9de-3e76-407d-e8bc-909bab701540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119598\n"
     ]
    }
   ],
   "source": [
    "#Load data \n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data('train.csv', batch_size,seq_length)\n",
    "vocab_size = len(corpus.dictionary) \n",
    "num_batches = ids.size(1) // seq_length \n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6aL_BmoFYN5"
   },
   "outputs": [],
   "source": [
    "train_model = Lstm(batch_size,vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# choose loss function: \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "#choose optimizer function \n",
    "optimizer = torch.optim.Adam(train_model.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.Adam(train_model.parameters(), lr=learning_rate * 0.1)\n",
    "optimizer3 = torch.optim.Adam(train_model.parameters(), lr=learning_rate * 0.05)\n",
    "optimizer4 = torch.optim.Adam(train_model.parameters(), lr=learning_rate * 0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2593
    },
    "colab_type": "code",
    "id": "MBveBtWcFYN9",
    "outputId": "ab1de9e3-5914-4648-ccc3-38c90ea8c727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current epoch [1/11], step_num in this epoch[0/2644], Loss: 11.6942,Perplexity: 119879.97\n",
      "the current epoch [1/11], step_num in this epoch[200/2644], Loss: 7.2569,Perplexity: 1417.85\n",
      "the current epoch [1/11], step_num in this epoch[400/2644], Loss: 6.8829,Perplexity: 975.44\n",
      "the current epoch [1/11], step_num in this epoch[600/2644], Loss: 7.4077,Perplexity: 1648.65\n",
      "the current epoch [1/11], step_num in this epoch[800/2644], Loss: 7.2093,Perplexity: 1351.90\n",
      "the current epoch [1/11], step_num in this epoch[1000/2644], Loss: 7.0277,Perplexity: 1127.46\n",
      "the current epoch [1/11], step_num in this epoch[1200/2644], Loss: 6.5548,Perplexity: 702.58\n",
      "the current epoch [1/11], step_num in this epoch[1400/2644], Loss: 6.4832,Perplexity: 654.05\n",
      "the current epoch [1/11], step_num in this epoch[1600/2644], Loss: 6.9085,Perplexity: 1000.71\n",
      "the current epoch [1/11], step_num in this epoch[1800/2644], Loss: 7.2468,Perplexity: 1403.64\n",
      "the current epoch [1/11], step_num in this epoch[2000/2644], Loss: 7.0404,Perplexity: 1141.82\n",
      "the current epoch [1/11], step_num in this epoch[2200/2644], Loss: 6.7360,Perplexity: 842.16\n",
      "the current epoch [1/11], step_num in this epoch[2400/2644], Loss: 6.7002,Perplexity: 812.54\n",
      "the current epoch [1/11], step_num in this epoch[2600/2644], Loss: 7.0732,Perplexity: 1179.94\n",
      "the current epoch [2/11], step_num in this epoch[0/2644], Loss: 6.6976,Perplexity: 810.45\n",
      "the current epoch [2/11], step_num in this epoch[200/2644], Loss: 6.1113,Perplexity: 450.92\n",
      "the current epoch [2/11], step_num in this epoch[400/2644], Loss: 5.8450,Perplexity: 345.50\n",
      "the current epoch [2/11], step_num in this epoch[600/2644], Loss: 6.3616,Perplexity: 579.19\n",
      "the current epoch [2/11], step_num in this epoch[800/2644], Loss: 6.1463,Perplexity: 467.00\n",
      "the current epoch [2/11], step_num in this epoch[1000/2644], Loss: 5.9134,Perplexity: 369.97\n",
      "the current epoch [2/11], step_num in this epoch[1200/2644], Loss: 5.6108,Perplexity: 273.35\n",
      "the current epoch [2/11], step_num in this epoch[1400/2644], Loss: 5.7317,Perplexity: 308.50\n",
      "the current epoch [2/11], step_num in this epoch[1600/2644], Loss: 6.0139,Perplexity: 409.08\n",
      "the current epoch [2/11], step_num in this epoch[1800/2644], Loss: 5.9722,Perplexity: 392.36\n",
      "the current epoch [2/11], step_num in this epoch[2000/2644], Loss: 6.1677,Perplexity: 477.10\n",
      "the current epoch [2/11], step_num in this epoch[2200/2644], Loss: 5.9141,Perplexity: 370.21\n",
      "the current epoch [2/11], step_num in this epoch[2400/2644], Loss: 5.8407,Perplexity: 344.01\n",
      "the current epoch [2/11], step_num in this epoch[2600/2644], Loss: 5.8419,Perplexity: 344.44\n",
      "the current epoch [3/11], step_num in this epoch[0/2644], Loss: 5.9584,Perplexity: 387.01\n",
      "the current epoch [3/11], step_num in this epoch[200/2644], Loss: 6.0750,Perplexity: 434.85\n",
      "the current epoch [3/11], step_num in this epoch[400/2644], Loss: 5.6376,Perplexity: 280.78\n",
      "the current epoch [3/11], step_num in this epoch[600/2644], Loss: 6.1492,Perplexity: 468.34\n",
      "the current epoch [3/11], step_num in this epoch[800/2644], Loss: 5.7548,Perplexity: 315.71\n",
      "the current epoch [3/11], step_num in this epoch[1000/2644], Loss: 5.7110,Perplexity: 302.16\n",
      "the current epoch [3/11], step_num in this epoch[1200/2644], Loss: 5.4518,Perplexity: 233.17\n",
      "the current epoch [3/11], step_num in this epoch[1400/2644], Loss: 5.4376,Perplexity: 229.89\n",
      "the current epoch [3/11], step_num in this epoch[1600/2644], Loss: 5.6168,Perplexity: 275.02\n",
      "the current epoch [3/11], step_num in this epoch[1800/2644], Loss: 5.5164,Perplexity: 248.74\n",
      "the current epoch [3/11], step_num in this epoch[2000/2644], Loss: 5.6619,Perplexity: 287.71\n",
      "the current epoch [3/11], step_num in this epoch[2200/2644], Loss: 5.2578,Perplexity: 192.05\n",
      "the current epoch [3/11], step_num in this epoch[2400/2644], Loss: 5.4043,Perplexity: 222.36\n",
      "the current epoch [3/11], step_num in this epoch[2600/2644], Loss: 4.9492,Perplexity: 141.06\n",
      "the current epoch [4/11], step_num in this epoch[0/2644], Loss: 5.2690,Perplexity: 194.23\n",
      "the current epoch [4/11], step_num in this epoch[200/2644], Loss: 5.3285,Perplexity: 206.13\n",
      "the current epoch [4/11], step_num in this epoch[400/2644], Loss: 5.0092,Perplexity: 149.78\n",
      "the current epoch [4/11], step_num in this epoch[600/2644], Loss: 5.3213,Perplexity: 204.65\n",
      "the current epoch [4/11], step_num in this epoch[800/2644], Loss: 4.8904,Perplexity: 133.01\n",
      "the current epoch [4/11], step_num in this epoch[1000/2644], Loss: 5.0259,Perplexity: 152.30\n",
      "the current epoch [4/11], step_num in this epoch[1200/2644], Loss: 4.8753,Perplexity: 131.02\n",
      "the current epoch [4/11], step_num in this epoch[1400/2644], Loss: 4.8530,Perplexity: 128.13\n",
      "the current epoch [4/11], step_num in this epoch[1600/2644], Loss: 4.9165,Perplexity: 136.53\n",
      "the current epoch [4/11], step_num in this epoch[1800/2644], Loss: 4.5719,Perplexity: 96.72\n",
      "the current epoch [4/11], step_num in this epoch[2000/2644], Loss: 4.9369,Perplexity: 139.34\n",
      "the current epoch [4/11], step_num in this epoch[2200/2644], Loss: 4.7228,Perplexity: 112.48\n",
      "the current epoch [4/11], step_num in this epoch[2400/2644], Loss: 4.9468,Perplexity: 140.73\n",
      "the current epoch [4/11], step_num in this epoch[2600/2644], Loss: 4.5781,Perplexity: 97.33\n",
      "the current epoch [5/11], step_num in this epoch[0/2644], Loss: 4.8387,Perplexity: 126.31\n",
      "the current epoch [5/11], step_num in this epoch[200/2644], Loss: 4.9117,Perplexity: 135.87\n",
      "the current epoch [5/11], step_num in this epoch[400/2644], Loss: 4.6889,Perplexity: 108.74\n",
      "the current epoch [5/11], step_num in this epoch[600/2644], Loss: 4.9045,Perplexity: 134.90\n",
      "the current epoch [5/11], step_num in this epoch[800/2644], Loss: 4.5188,Perplexity: 91.72\n",
      "the current epoch [5/11], step_num in this epoch[1000/2644], Loss: 4.7391,Perplexity: 114.33\n",
      "the current epoch [5/11], step_num in this epoch[1200/2644], Loss: 4.6633,Perplexity: 105.98\n",
      "the current epoch [5/11], step_num in this epoch[1400/2644], Loss: 4.6419,Perplexity: 103.74\n",
      "the current epoch [5/11], step_num in this epoch[1600/2644], Loss: 4.6967,Perplexity: 109.58\n",
      "the current epoch [5/11], step_num in this epoch[1800/2644], Loss: 4.3234,Perplexity: 75.45\n",
      "the current epoch [5/11], step_num in this epoch[2000/2644], Loss: 4.6650,Perplexity: 106.16\n",
      "the current epoch [5/11], step_num in this epoch[2200/2644], Loss: 4.5587,Perplexity: 95.46\n",
      "the current epoch [5/11], step_num in this epoch[2400/2644], Loss: 4.7608,Perplexity: 116.84\n",
      "the current epoch [5/11], step_num in this epoch[2600/2644], Loss: 4.4332,Perplexity: 84.20\n",
      "the current epoch [6/11], step_num in this epoch[0/2644], Loss: 4.6128,Perplexity: 100.77\n",
      "the current epoch [6/11], step_num in this epoch[200/2644], Loss: 4.6841,Perplexity: 108.21\n",
      "the current epoch [6/11], step_num in this epoch[400/2644], Loss: 4.5059,Perplexity: 90.55\n",
      "the current epoch [6/11], step_num in this epoch[600/2644], Loss: 4.6889,Perplexity: 108.73\n",
      "the current epoch [6/11], step_num in this epoch[800/2644], Loss: 4.3541,Perplexity: 77.80\n",
      "the current epoch [6/11], step_num in this epoch[1000/2644], Loss: 4.5736,Perplexity: 96.90\n",
      "the current epoch [6/11], step_num in this epoch[1200/2644], Loss: 4.5599,Perplexity: 95.57\n",
      "the current epoch [6/11], step_num in this epoch[1400/2644], Loss: 4.5335,Perplexity: 93.09\n",
      "the current epoch [6/11], step_num in this epoch[1600/2644], Loss: 4.5866,Perplexity: 98.16\n",
      "the current epoch [6/11], step_num in this epoch[1800/2644], Loss: 4.2185,Perplexity: 67.93\n",
      "the current epoch [6/11], step_num in this epoch[2000/2644], Loss: 4.5450,Perplexity: 94.16\n",
      "the current epoch [6/11], step_num in this epoch[2200/2644], Loss: 4.4762,Perplexity: 87.90\n",
      "the current epoch [6/11], step_num in this epoch[2400/2644], Loss: 4.6476,Perplexity: 104.33\n",
      "the current epoch [6/11], step_num in this epoch[2600/2644], Loss: 4.3680,Perplexity: 78.89\n",
      "the current epoch [7/11], step_num in this epoch[0/2644], Loss: 4.4760,Perplexity: 87.88\n",
      "the current epoch [7/11], step_num in this epoch[200/2644], Loss: 4.6637,Perplexity: 106.02\n",
      "the current epoch [7/11], step_num in this epoch[400/2644], Loss: 4.4905,Perplexity: 89.17\n",
      "the current epoch [7/11], step_num in this epoch[600/2644], Loss: 4.7422,Perplexity: 114.68\n",
      "the current epoch [7/11], step_num in this epoch[800/2644], Loss: 4.4321,Perplexity: 84.11\n",
      "the current epoch [7/11], step_num in this epoch[1000/2644], Loss: 4.5963,Perplexity: 99.12\n",
      "the current epoch [7/11], step_num in this epoch[1200/2644], Loss: 4.6574,Perplexity: 105.36\n",
      "the current epoch [7/11], step_num in this epoch[1400/2644], Loss: 4.6079,Perplexity: 100.28\n",
      "the current epoch [7/11], step_num in this epoch[1600/2644], Loss: 4.6917,Perplexity: 109.04\n",
      "the current epoch [7/11], step_num in this epoch[1800/2644], Loss: 4.4108,Perplexity: 82.33\n",
      "the current epoch [7/11], step_num in this epoch[2000/2644], Loss: 4.6556,Perplexity: 105.17\n",
      "the current epoch [7/11], step_num in this epoch[2200/2644], Loss: 4.5229,Perplexity: 92.11\n",
      "the current epoch [7/11], step_num in this epoch[2400/2644], Loss: 4.7180,Perplexity: 111.94\n",
      "the current epoch [7/11], step_num in this epoch[2600/2644], Loss: 4.4531,Perplexity: 85.89\n",
      "the current epoch [8/11], step_num in this epoch[0/2644], Loss: 4.2524,Perplexity: 70.27\n",
      "the current epoch [8/11], step_num in this epoch[200/2644], Loss: 4.4981,Perplexity: 89.84\n",
      "the current epoch [8/11], step_num in this epoch[400/2644], Loss: 4.3285,Perplexity: 75.83\n",
      "the current epoch [8/11], step_num in this epoch[600/2644], Loss: 4.5493,Perplexity: 94.56\n",
      "the current epoch [8/11], step_num in this epoch[800/2644], Loss: 4.2655,Perplexity: 71.20\n",
      "the current epoch [8/11], step_num in this epoch[1000/2644], Loss: 4.4227,Perplexity: 83.32\n",
      "the current epoch [8/11], step_num in this epoch[1200/2644], Loss: 4.4861,Perplexity: 88.78\n",
      "the current epoch [8/11], step_num in this epoch[1400/2644], Loss: 4.4621,Perplexity: 86.67\n",
      "the current epoch [8/11], step_num in this epoch[1600/2644], Loss: 4.5271,Perplexity: 92.49\n",
      "the current epoch [8/11], step_num in this epoch[1800/2644], Loss: 4.2340,Perplexity: 68.99\n",
      "the current epoch [8/11], step_num in this epoch[2000/2644], Loss: 4.4975,Perplexity: 89.79\n",
      "the current epoch [8/11], step_num in this epoch[2200/2644], Loss: 4.4175,Perplexity: 82.89\n",
      "the current epoch [8/11], step_num in this epoch[2400/2644], Loss: 4.6076,Perplexity: 100.25\n",
      "the current epoch [8/11], step_num in this epoch[2600/2644], Loss: 4.3729,Perplexity: 79.27\n",
      "the current epoch [9/11], step_num in this epoch[0/2644], Loss: 4.2503,Perplexity: 70.12\n",
      "the current epoch [9/11], step_num in this epoch[200/2644], Loss: 4.4362,Perplexity: 84.45\n",
      "the current epoch [9/11], step_num in this epoch[400/2644], Loss: 4.2732,Perplexity: 71.75\n",
      "the current epoch [9/11], step_num in this epoch[600/2644], Loss: 4.4845,Perplexity: 88.63\n",
      "the current epoch [9/11], step_num in this epoch[800/2644], Loss: 4.2149,Perplexity: 67.69\n",
      "the current epoch [9/11], step_num in this epoch[1000/2644], Loss: 4.3809,Perplexity: 79.91\n",
      "the current epoch [9/11], step_num in this epoch[1200/2644], Loss: 4.4436,Perplexity: 85.08\n",
      "the current epoch [9/11], step_num in this epoch[1400/2644], Loss: 4.4155,Perplexity: 82.72\n",
      "the current epoch [9/11], step_num in this epoch[1600/2644], Loss: 4.4815,Perplexity: 88.37\n",
      "the current epoch [9/11], step_num in this epoch[1800/2644], Loss: 4.1866,Perplexity: 65.80\n",
      "the current epoch [9/11], step_num in this epoch[2000/2644], Loss: 4.4474,Perplexity: 85.40\n",
      "the current epoch [9/11], step_num in this epoch[2200/2644], Loss: 4.3883,Perplexity: 80.50\n",
      "the current epoch [9/11], step_num in this epoch[2400/2644], Loss: 4.5622,Perplexity: 95.79\n",
      "the current epoch [9/11], step_num in this epoch[2600/2644], Loss: 4.3393,Perplexity: 76.66\n",
      "the current epoch [10/11], step_num in this epoch[0/2644], Loss: 4.2525,Perplexity: 70.28\n",
      "the current epoch [10/11], step_num in this epoch[200/2644], Loss: 4.4991,Perplexity: 89.94\n",
      "the current epoch [10/11], step_num in this epoch[400/2644], Loss: 4.3262,Perplexity: 75.66\n",
      "the current epoch [10/11], step_num in this epoch[600/2644], Loss: 4.6270,Perplexity: 102.21\n",
      "the current epoch [10/11], step_num in this epoch[800/2644], Loss: 4.3607,Perplexity: 78.31\n",
      "the current epoch [10/11], step_num in this epoch[1000/2644], Loss: 4.4722,Perplexity: 87.55\n",
      "the current epoch [10/11], step_num in this epoch[1200/2644], Loss: 4.5790,Perplexity: 97.42\n",
      "the current epoch [10/11], step_num in this epoch[1400/2644], Loss: 4.4839,Perplexity: 88.58\n",
      "the current epoch [10/11], step_num in this epoch[1600/2644], Loss: 4.5768,Perplexity: 97.20\n",
      "the current epoch [10/11], step_num in this epoch[1800/2644], Loss: 4.3205,Perplexity: 75.23\n",
      "the current epoch [10/11], step_num in this epoch[2000/2644], Loss: 4.5365,Perplexity: 93.36\n",
      "the current epoch [10/11], step_num in this epoch[2200/2644], Loss: 4.4512,Perplexity: 85.73\n",
      "the current epoch [10/11], step_num in this epoch[2400/2644], Loss: 4.6234,Perplexity: 101.84\n",
      "the current epoch [10/11], step_num in this epoch[2600/2644], Loss: 4.4176,Perplexity: 82.90\n",
      "the current epoch [11/11], step_num in this epoch[0/2644], Loss: 4.1578,Perplexity: 63.93\n",
      "the current epoch [11/11], step_num in this epoch[200/2644], Loss: 4.3908,Perplexity: 80.70\n",
      "the current epoch [11/11], step_num in this epoch[400/2644], Loss: 4.2381,Perplexity: 69.28\n",
      "the current epoch [11/11], step_num in this epoch[600/2644], Loss: 4.4995,Perplexity: 89.97\n",
      "the current epoch [11/11], step_num in this epoch[800/2644], Loss: 4.2418,Perplexity: 69.53\n",
      "the current epoch [11/11], step_num in this epoch[1000/2644], Loss: 4.3497,Perplexity: 77.46\n",
      "the current epoch [11/11], step_num in this epoch[1200/2644], Loss: 4.4643,Perplexity: 86.86\n",
      "the current epoch [11/11], step_num in this epoch[1400/2644], Loss: 4.3905,Perplexity: 80.68\n",
      "the current epoch [11/11], step_num in this epoch[1600/2644], Loss: 4.4722,Perplexity: 87.54\n",
      "the current epoch [11/11], step_num in this epoch[1800/2644], Loss: 4.2106,Perplexity: 67.40\n",
      "the current epoch [11/11], step_num in this epoch[2000/2644], Loss: 4.4609,Perplexity: 86.57\n",
      "the current epoch [11/11], step_num in this epoch[2200/2644], Loss: 4.4213,Perplexity: 83.20\n",
      "the current epoch [11/11], step_num in this epoch[2400/2644], Loss: 4.5944,Perplexity: 98.93\n",
      "the current epoch [11/11], step_num in this epoch[2600/2644], Loss: 4.3967,Perplexity: 81.18\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation process \"truncated\" (do not copy gradient)\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states] \n",
    "\n",
    "# training...\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "    h_pre = torch.zeros(batch_size,hidden_size).to(device)\n",
    "    c_pre = torch.zeros(batch_size,hidden_size).to(device)     \n",
    "    loss = 0.0\n",
    "    \n",
    "    if (epoch ==num_epochs1+num_epochs2 +num_epochs3):\n",
    "        torch.save(train_model.state_dict(), 'lstm_epoc_3.ckpt')\n",
    "        optimizer = optimizer4\n",
    "      \n",
    "    if (epoch ==num_epochs1+num_epochs2):\n",
    "        torch.save(train_model.state_dict(), 'lstm_epoc_2.ckpt')\n",
    "        optimizer = optimizer3\n",
    "        \n",
    "    elif (epoch ==num_epochs1):\n",
    "        torch.save(train_model.state_dict(), 'lstm_epoc_1.ckpt')\n",
    "        optimizer = optimizer2\n",
    "        \n",
    "    for i in range(0, ids.size(1)-1 , seq_length):##  总的列数减1，间隔一句话的长度,0,25,50....\n",
    "        loss = 0.0\n",
    "        inputs = ids[:,i:i+seq_length-1].to(device)  #  [batch_size,seq_length-1]:[20,24]\n",
    "        inputs = inputs.t() # [24,20]\n",
    "        targets = ids[:,(i+1):i+seq_length].to(device) # Output is delayed by one word after the input\n",
    "        outputs,(h_t,c_t) = train_model(inputs,h_pre,c_pre)\n",
    "        loss = criterion(outputs,targets.reshape(-1))  \n",
    "        \n",
    "        # the 3 key steps\n",
    "        train_model.zero_grad()\n",
    "        loss.backward( )\n",
    "        clip_grad_norm_(train_model.parameters(),0.5)  # max norm of the gradients to avoid gradient explode\n",
    "        optimizer.step()\n",
    "        \n",
    "        step = (i+1) // seq_length\n",
    "        if step % 200 == 0:\n",
    "            print ('the current epoch [{}/{}], step_num in this epoch[{}/{}], Loss: {:.4f},Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))\n",
    "            \n",
    "      \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mp0mU-kANTvu"
   },
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\" : train_model.state_dict(),\n",
    "            \"optimizer_state_dict\" : optimizer.state_dict()},\n",
    "            'lstm_epoc_save.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "0FeLiWQUNhrI",
    "outputId": "94266b15-bfc0-4ed1-e1da-eb2a029a4818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lstm(\n",
      "  (embed): Embedding(119598, 128)\n",
      "  (linear): Linear(in_features=1024, out_features=119598, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('lstm_epoc_save.ckpt')\n",
    "lstm_test_model = Lstm(batch_size,vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "lstm_test_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "print(lstm_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "colab_type": "code",
    "id": "BMaqT_e2MSiM",
    "outputId": "29d91e82-dc60-4e5e-a20d-ed2dfb2c053e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate [100/1000] words，save in sample_0609.txt\n",
      "generate [200/1000] words，save in sample_0609.txt\n",
      "generate [300/1000] words，save in sample_0609.txt\n",
      "generate [400/1000] words，save in sample_0609.txt\n",
      "generate [500/1000] words，save in sample_0609.txt\n",
      "generate [600/1000] words，save in sample_0609.txt\n",
      "generate [700/1000] words，save in sample_0609.txt\n",
      "generate [800/1000] words，save in sample_0609.txt\n",
      "generate [900/1000] words，save in sample_0609.txt\n",
      "generate [1000/1000] words，save in sample_0609.txt\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    num_samples = 1000\n",
    "    with open('sample_0609.txt', 'w') as f:\n",
    "         # initialize\n",
    "        (h,c) = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
    "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
    "\n",
    "        # Randomly select a word as input\n",
    "        prob = torch.ones(vocab_size)\n",
    "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            output, (h,c)  = lstm_test_model(input, h,c) \n",
    "            # predictions\n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "\n",
    "            # Fill in preditions(input data for the next prediticon)\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # write the predictions \n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('generate [{}/{}] words，save in {}'.format(i+1, num_samples, 'sample_0609.txt'))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_CxDg98G6O0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geKgX-6Thxnl"
   },
   "source": [
    "writer: @ hebaodan"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_main_ifoc .ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
